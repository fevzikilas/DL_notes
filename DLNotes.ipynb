{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326a9b81-4f68-4e5a-89f5-7ed64cad533a",
   "metadata": {},
   "source": [
    "# Deep Learning Notes\n",
    "\n",
    "##### **Book**: Deep Learning by Ian Goodfellow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31be1d9-f833-46a4-a45d-ca785b567f4b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467d460-27b2-4fd5-8df7-6f48f2f8a4d2",
   "metadata": {},
   "source": [
    "Genel olarak ai tarihi ve neden DL ortaya cikmis ondan bahsediyor. Ilk zamanlardan bazi ornekler verilmis. Satranc orneginden bahsediliyor. Knowledge base approach ise makinenin logical inference rule kullanarak otamatik olarak ifade olusturabilmesine deniyor. Ama bu yontem basariya ulasmamis. \n",
    "\n",
    "Cyc ornegide gozuktugu gibi traş olan insanın makine tarafından algilanamamasi ve hata vermesinden oturu bu makinelerin kendi bilgilerini edinme yeteneği lazimdi. Buna Machine Learning denilmiş. ML sayesinde makineleri gercek dunya problemlerinin yer aldigi problemelri cozulmesi saglandi. Ve logistic regression, naive Bayes gibi algorithmalar ornek gosterilmis.\n",
    "\n",
    "Basit ML algoritmalari verinin nasil tanimlandigina gore cok degisir. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487c237-1f28-4282-8b0b-c482fffe9414",
   "metadata": {},
   "source": [
    "## 1. Introduction (Giriş)\n",
    "Derin öğrenmenin temeli: Bilgisayarların karmaşık problemleri çözebilmesi için deneyimlerden öğrenmesi gerektiği ve dünyayı kavraması için kavramsal hiyerarşiler oluşturması gerektiği anlatılıyor.\n",
    "Kime hitap ediyor: Hem üniversite öğrencileri hem de yazılım mühendisleri hedeflenmiş. Derin öğrenme hakkında teorik bilgi edinmek isteyenler ve uygulama yapmak isteyenler için."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f5a21-9150-41f0-9c34-fa3438989461",
   "metadata": {},
   "source": [
    "1. Bilgisayarların Öğrenmesi Gerekiyor\n",
    "Kavramsal Hiyerarşi: İnsanların zor bir problemi sezgisel olarak çözmeleri, bilgisayarlar için zorlayıcıdır. Derin öğrenme, karmaşık kavramları daha basit kavramların üzerine inşa ederek bu sezgisel problemlere çözüm getirir.\n",
    "Derin Öğrenmenin Temeli: Bilgisayarlar, insan müdahalesi olmadan deneyimlerden öğrenerek bilgileri kavramsal hiyerarşiler şeklinde yapılandırabilir. Bu kavramlar, basit unsurların daha karmaşık formlara dönüşmesiyle oluşur. Örneğin, bir nesneyi tanımak için önce kenarları, sonra köşeleri, ardından nesne parçalarını öğrenmek gibi.\n",
    "2. Zorlukların Kaynağı: Temel Bilgiyi Aktarmak\n",
    "Derin öğrenmeden önce, yapay zeka alanında insanlar bilgisayarlara bilgi vermek için kurallar ve formel diller kullanıyordu (örneğin, Cyc projesi). Ancak, bu sistemler gerçek dünya bilgisini anlamakta zorlanıyordu.\n",
    "Bilginin kodlanması zorlukları: Bilgisayarlara dünyaya dair bilgileri formel kurallarla anlatmak zor ve karmaşık. Cyc gibi projeler insan müdahalesine aşırı bağımlıydı ve bu yüzden başarıya ulaşamadı.\n",
    "Makine öğrenimi ve derin öğrenme ile çözümler: Bilgisayarlar, insan müdahalesi olmadan ham verilerden düzenleri keşfederek bilgi öğrenebilir hale gelir. Derin öğrenmenin temel avantajı da burada yatar: Bilgiyi ham verilerden öğrenmek.\n",
    "3. Temsil Öğrenme ve Derin Öğrenme\n",
    "Temsilin Önemi: Makine öğreniminde algoritmaların performansı, verilerin nasıl temsil edildiğine çok bağlıdır. Yanlış veya eksik özellikler ile çalışan modeller başarısız olabilir. Örneğin, bir doktorun hastanın tıbbi geçmişinden aldığı bilgilere göre karar vermesi, MRI taraması gibi ham verilerle karar vermesinden daha kolaydır.\n",
    "Temsil Öğrenme: Derin öğrenme, bilgisayarlara verilerden hangi özelliklerin çıkarılacağını öğretir. Yani, algoritma veriden anlamlı temsilleri öğrenir ve bu temsiller daha sonra başka görevlerde kullanılabilir.\n",
    "Otoenkodlayıcılar: Temsil öğrenmenin en tipik örneğidir. Otoenkodlayıcılar, girdileri başka bir temsile dönüştürerek yeni özellikler öğrenir. Amaç, verilerdeki karmaşık yapıları öğrenerek daha basit temsillere dönüştürmektir.\n",
    "4. Farklılıkların Ayrıştırılması\n",
    "Verilerdeki farklı faktörler: Bir görüntüdeki ya da ses kaydındaki çeşitli faktörler (konuşmacının aksanı, yaş, ışık koşulları) algıyı zorlaştırabilir. Bu faktörleri ayrıştırmak, bilgisayarların başarılı bir şekilde karar vermesine yardımcı olur.\n",
    "Derin öğrenmenin katkısı: Derin öğrenme, karmaşık verilerdeki bu tür faktörleri ayrıştırarak, doğru olanları kullanıp gereksiz olanları ayıklamayı başarır. Örneğin, bir fotoğraftaki araba gölgelerden ya da güneş ışığından etkilenmiş olabilir, ancak model, bu faktörleri dikkate alarak arabayı tanıyabilir.\n",
    "5. Derin Öğrenme Modeli Nasıl Çalışır?\n",
    "Öğrenme Aşamaları: Derin öğrenme sistemleri, verilerdeki basit özelliklerden başlayarak giderek daha karmaşık temsilleri oluşturur. İlk katman, kenarları tanır; ikinci katman, kenarları birleştirerek köşeleri tanır; son katman ise nesneleri tanır (örn. bir araba ya da insan).\n",
    "Gizli Katmanlar: Görüntüdeki kenarları tanımak gibi basit görevlerle başlar ve bu görevler daha yüksek seviyedeki görevlerle birleştirilir. Bu nedenle, gizli katmanlar olarak adlandırılan bu katmanlar, verilerdeki soyut kavramları keşfeder.\n",
    "6. Derinlik Kavramı\n",
    "Modelin Derinliği: Derin öğrenme modellerinin derinliği, işlem adımlarının sayısıyla ölçülür. Bir model ne kadar derinse, o kadar fazla işlem basamağı kullanılır ve daha karmaşık yapılar öğrenilir. Bu, modelin kapasitesini ve esnekliğini artırır.\n",
    "Derinlikle Gelen Güç: Daha derin modeller, daha karmaşık kavramları öğrenebilir ve bu sayede bir dizi işlemi ardışık olarak gerçekleştirebilir.\n",
    "7. Tarihsel Gelişim\n",
    "Sibernetikten Günümüze: Derin öğrenmenin kökenleri 1940'lara kadar dayanır. \"Sibernetik\", \"bağlantıcılık\" ve \"derin öğrenme\" isimleriyle bilinen üç ana gelişim dalgası vardır.\n",
    "Derin Öğrenmenin Evrimi: Derin öğrenme, büyük veri setlerinin artması ve bilgisayar altyapısının gelişmesiyle daha yaygın hale geldi. Özellikle 2006'dan itibaren popülerlik kazanmaya başlamıştır.\n",
    "Bu giriş bölümü, derin öğrenmenin tarihsel arka planını, temel kavramlarını ve günümüz AI problemlerine nasıl çözüm sunduğunu açıklar. Derin öğrenmenin en büyük katkısı, ham verilerden anlamlı temsiller çıkarabilmesi ve bu temsilleri kullanarak gerçek dünya problemlerini çözmesidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9139b-0219-41b4-99e8-452dc8d32944",
   "metadata": {},
   "source": [
    "## 2. Applied Math and Machine Learning Basics (Uygulamalı Matematik ve Makine Öğrenimi Temelleri)\n",
    "Lineer Cebir (Linear Algebra) derin öğrenme modellerinin temellerini anlamak için gerekli olan matematiksel araçları tanıtır. Bu bölümde, vektörler, matrisler ve tensörler gibi temel yapı taşlarından başlayarak, daha karmaşık cebirsel işlemler ve kavramlar işlenir. Derin öğrenmenin yapı taşları olan lineer cebir, özellikle çok boyutlu veriyle çalışırken ve parametreleri optimize ederken kritik bir rol oynar. İşte detaylar:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03662695-bc53-46a9-ad45-357b592e8241",
   "metadata": {},
   "source": [
    "2.1 Skalerler, Vektörler, Matrisler ve Tensörler\n",
    "Skalerler: Tek bir sayı (örneğin, bir reel sayı) skaler olarak adlandırılır.\n",
    "Vektörler: Bir dizi sayıyı bir arada tutan yapıdır ve genellikle bir sütun veya satır halinde gösterilir. Örneğin, bir modelin ağırlıklarını ya da bir veri noktasını temsil edebilir.\n",
    "Matrisler: İki boyutlu sayı dizisidir (satırlar ve sütunlar). Derin öğrenmede sıkça kullanılan yapılar arasındadır, özellikle bir veri setini veya bir ağırlık matrisini ifade etmek için.\n",
    "Tensörler: Üç veya daha fazla boyuta sahip matrislerdir. Örneğin, bir renkli görüntüdeki her piksel üç boyutlu (RGB) bir tensörle temsil edilebilir.\n",
    "Bu yapıların her biri, derin öğrenmede girdileri, parametreleri ve çıktıları temsil etmek için kullanılır. Tensörler, derin öğrenmede en genel veri yapısıdır çünkü hem girişler (örneğin, görüntüler) hem de ağırlıklar tensörler olarak ifade edilir.\n",
    "\n",
    "2.2 Matris ve Vektör Çarpımı\n",
    "Vektör-Çarpım: Bir vektör ile bir skalerin çarpımı, tüm vektör elemanlarının o skalerle çarpılması anlamına gelir. Benzer şekilde, bir matris ile bir vektör çarpıldığında, matrisin her bir satırı vektörle çarpılır.\n",
    "Matris-Matris Çarpımı: İki matrisin çarpımı, birinci matrisin satırları ile ikinci matrisin sütunlarının noktasal çarpımlarının alınması ile yapılır. Bu işlem, özellikle katmanlar arasındaki sinirsel bağlantıları tanımlarken kullanılır.\n",
    "Matris ve vektör çarpımları, derin öğrenmede nöronların çıktılarının hesaplanmasında kritik öneme sahiptir.\n",
    "\n",
    "2.3 Birim Matris ve Ters Matrisler\n",
    "Birim Matris (Identity Matrix): Çarpıldığında sonucu değiştirmeyen matristir (örn. birim matris ile bir vektörü çarptığınızda vektör değişmez). Sinir ağlarında genellikle başlangıç değerleri ve bazı özel yapıların korunmasında kullanılır.\n",
    "Ters Matris (Inverse Matrix): Bir matrisin tersini almak, o matrisle çarpıldığında birim matris veren matris demektir. Bazı optimizasyon problemlerinde matris tersine ihtiyaç duyulur, ancak her matrisin tersi bulunmayabilir.\n",
    "\n",
    "2.4 Lineer Bağımlılık ve Span\n",
    "Lineer Bağımlılık: Eğer bir vektör, diğer vektörlerin bir lineer kombinasyonu olarak ifade edilebiliyorsa, bu vektörler lineer bağımlıdır. Derin öğrenme modellerinde, lineer bağımlı özelliklerin modellenmesi genellikle zorluk çıkarabilir çünkü bu, fazladan bilgi içermeyen ya da öğrenmeyi zorlaştıran durumlar yaratır.\n",
    "Span: Bir vektör kümesinin span’i, o vektörlerin oluşturduğu tüm olası lineer kombinasyonların oluşturduğu vektör uzayıdır.\n",
    "\n",
    "2.5 Normlar\n",
    "Normlar, vektörlerin büyüklüğünü ölçer ve genellikle iki tür norm kullanılır:\n",
    "L1 Normu: Bir vektördeki her elemanın mutlak değerlerinin toplamıdır.\n",
    "L2 Normu (Öklid Normu): Bir vektördeki her elemanın karelerinin toplamının kareköküdür.\n",
    "Normların Önemi: Sinir ağlarında ağırlıkları düzenlemek ve öğrenme sürecini kontrol etmek için normlar kullanılır. Özellikle regülerizasyon yöntemlerinde L2 normu sıkça kullanılır.\n",
    "\n",
    "2.6 Özel Matris ve Vektör Türleri\n",
    "Sıfır Matris: Tüm elemanları sıfır olan matristir.\n",
    "Diyagonal Matris: Sadece köşegen üzerindeki elemanları sıfırdan farklı olan matristir.\n",
    "Simetrik Matris: Transpozu kendisine eşit olan matris olarak tanımlanır.\n",
    "\n",
    "2.7 Özdekompozisyon (Eigendecomposition)\n",
    "Özdeğerler ve Özvektörler: Bir matrisin, özvektörlerle ifade edilebilmesi durumu, özdekompozisyon olarak bilinir. Özdeğerler ve özvektörler, bir matrisin temel yapı taşlarını anlamak için kullanılır ve derin öğrenmede veri sıkıştırma (özellikle PCA) ve model optimizasyonu gibi alanlarda kullanılır.\n",
    "\n",
    "2.8 Tekil Değer Ayrışımı (Singular Value Decomposition - SVD)\n",
    "SVD, bir matrisin üç farklı matrisin çarpımına ayrıştırılmasını sağlar. Bu, çok boyutlu verilerin daha basit yapılara indirgenmesinde önemli bir rol oynar. Verilerin sıkıştırılması ve boyut indirgeme teknikleri bu yöntemle yapılır.\n",
    "\n",
    "2.9 Moore-Penrose Pseudoinverse\n",
    "Terslenemeyen matrisler için kullanılan bu yöntem, özellikle optimizasyon problemlerinde (az sayıda veri noktası ile çok fazla parametre çalıştırıldığında) kullanılır.\n",
    "\n",
    "2.10 İz Operatörü (Trace Operator)\n",
    "Bir matrisin köşegendeki elemanlarının toplamını verir. Özellikle optimizasyon fonksiyonlarında ve matriksal diferansiyel denklemlerde kullanılır.\n",
    "\n",
    "2.11 Determinant\n",
    "Bir matrisin determinantı, matrisin çözüm uzayı hakkında bilgi verir ve özellikle ters matrislerin var olup olmadığını belirlemek için kullanılır.\n",
    "\n",
    "2.12 Örnek: Ana Bileşenler Analizi (PCA)\n",
    "PCA, verilerdeki varyansı en iyi açıklayan bileşenleri bulmak için kullanılan bir tekniktir. Bu yöntemde, verilerin boyutu azaltılırken en anlamlı olan özellikler korunur. PCA, genellikle veri sıkıştırma ve görselleştirme amacıyla kullanılır.\n",
    "Genel Yorum:\n",
    "Bu bölüm, derin öğrenme modellerinde kullanılan temel lineer cebir işlemlerini ve kavramlarını kapsamlı bir şekilde ele alır. Derin öğrenmede, veriler genellikle yüksek boyutludur ve bu yüzden lineer cebir, verilerin işlenmesinde ve modellerin eğitilmesinde kritik bir rol oynar. Kitabın bu bölümü, vektör ve matris işlemlerini derinlemesine anlaman için gerekli olan matematiksel altyapıyı sağlar. Özellikle optimizasyon problemlerinde ve çok boyutlu veri işlemede bu temel bilgileri kullanman gerekecek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b79e3-ab40-4186-bf5d-344f4977e3d6",
   "metadata": {},
   "source": [
    "## 3. Deep Networks: Modern Practices (Derin Ağlar: Modern Uygulamalar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11388e0e-b090-403f-b4bb-9ae4cc621246",
   "metadata": {},
   "source": [
    "Olasılık ve Bilgi Teorisi (Probability and Information Theory), makine öğreniminde belirsizliklerin nasıl ele alındığını ve bilgi teorisinin nasıl uygulandığını anlatır. Olasılık teorisi, modellerin tahmin yaparken belirsizlikle başa çıkmasını sağlar, bilgi teorisi ise modellerin veriyi nasıl sıkıştırabileceğini ve veriden nasıl bilgi çıkarabileceğini ele alır. Bölümün öne çıkan noktaları:\n",
    "\n",
    "3.1 Neden Olasılık?\n",
    "Olasılık, belirsizlik altında karar vermek için kritik bir araçtır. Verilerdeki belirsizlikler (gürültü veya eksik veriler) nedeniyle, tahminler her zaman deterministik olamaz.\n",
    "Makine öğreniminde olasılık, bir modelin çeşitli olasılıklara dayalı kararlar vermesini sağlar. Bu nedenle, olasılıklar hem eğitim sırasında hem de tahmin yaparken kullanılır.\n",
    "\n",
    "3.2 Rastgele Değişkenler\n",
    "Rastgele değişkenler, bir deneyin sonuçlarına bağlı olarak değişen değerlerdir. Olasılık teorisinin temeli olarak, rastgele değişkenler belirli bir veri kümesindeki belirsizlikleri modellemek için kullanılır.\n",
    "Sürekli ve ayrık rastgele değişkenler, belirli bir aralıkta veya belirli ayrık noktalarda olasılık dağılımlarına sahiptir.\n",
    "\n",
    "3.3 Olasılık Dağılımları\n",
    "Bir rastgele değişkenin alabileceği olası değerlerin olasılıklarını tanımlar. Örneğin, normal dağılım gibi yaygın olasılık dağılımları, model parametrelerinin nasıl dağıldığını ifade eder.\n",
    "\n",
    "3.4 Marjinal Olasılık\n",
    "Marjinal olasılık, tüm olası değerler üzerinden entegre edilen veya toplamda toplanan bir rastgele değişkenin olasılığıdır. Birden fazla değişkenin bulunduğu modellerde, ilgilenilen değişkenin olasılığını elde etmek için diğer değişkenlerin etkisi marjinalize edilir.\n",
    "\n",
    "3.5 Koşullu Olasılık\n",
    "Bir olayın, başka bir olayın gerçekleşmiş olduğu bilgisine dayanarak olasılığını hesaplar. Bu kavram, özellikle bağımsız olmayan değişkenlerle çalışırken önemlidir.\n",
    "\n",
    "3.6 Koşullu Olasılıkların Zincir Kuralı\n",
    "Koşullu olasılıkların hesaplanmasında kullanılan bir yöntemdir. Örneğin, birkaç bağımlı değişkenin olasılığı, bu zincir kuralı kullanılarak hesaplanır.\n",
    "\n",
    "3.7 Bağımsızlık ve Koşullu Bağımsızlık\n",
    "İki olay birbirinden bağımsızsa, birinin gerçekleşmesi diğerinin olasılığını etkilemez. Koşullu bağımsızlık ise, belirli bir üçüncü olay göz önüne alındığında iki olayın birbirinden bağımsız olmasını ifade eder.\n",
    "\n",
    "3.8 Beklenti, Varyans ve Kovaryans\n",
    "Beklenen değer (Expectation), bir rastgele değişkenin uzun vadeli ortalamasıdır.\n",
    "Varyans (Variance), bir rastgele değişkenin beklenen değer etrafında ne kadar dağıldığını ölçer.\n",
    "Kovaryans (Covariance), iki rastgele değişkenin ne kadar birlikte değiştiğini ölçer. İki değişken arasında pozitif veya negatif bir ilişki olup olmadığını gösterir.\n",
    "\n",
    "3.9 Yaygın Olasılık Dağılımları\n",
    "Bernoulli Dağılımı: Ayrık olaylar için kullanılır; olayın iki olası sonucu (başarı ya da başarısızlık) vardır.\n",
    "Gaussian (Normal) Dağılımı: Sürekli veriler için kullanılır ve birçok doğal olayda karşılaşılan simetrik bir dağılımdır.\n",
    "Çokterimli (Multinomial) Dağılım: Birden fazla kategorik sonuca sahip olaylar için kullanılır.\n",
    "Eksponansiyel ve Poisson Dağılımları: Belirli zaman aralıklarında gerçekleşen olayları modellemek için kullanılır.\n",
    "\n",
    "3.10 Yaygın Fonksiyonların Özellikleri\n",
    "Belirli olasılık dağılımlarında yaygın olarak kullanılan bazı fonksiyonlar (sigmoid, softmax) ve bu fonksiyonların özellikleri anlatılır.\n",
    "\n",
    "3.11 Bayes Kuralı\n",
    "Bayes Kuralı, bir olayın olasılığını, o olaya ilişkin önceki bilgiye dayanarak güncellemek için kullanılır. Bu, özellikle derin öğrenmede belirsizliği modellemek için kullanışlıdır.\n",
    "\n",
    "3.12 Sürekli Değişkenlerin Teknik Detayları\n",
    "Sürekli değişkenler için entegral hesaplamaları ve olasılık yoğunluk fonksiyonları anlatılır. Bu, özellikle sürekli veri kümelerinde olasılıkların nasıl ele alınacağını anlamak için önemlidir.\n",
    "\n",
    "3.13 Bilgi Teorisi\n",
    "Bilgi Entropisi: Bir rastgele değişkenin belirsizliğini ölçer. Yüksek entropi, daha fazla belirsizlik anlamına gelir.\n",
    "Kullback-Leibler Divergence (KL Divergence): İki olasılık dağılımı arasındaki farkı ölçer. Makine öğreniminde, bir modelin tahmin ettiği dağılımla gerçek dağılım arasındaki farkı ölçmek için kullanılır.\n",
    "Karşılıklı Bilgi: İki değişkenin birbirleri hakkında ne kadar bilgi taşıdığını ölçer.\n",
    "\n",
    "3.14 Yapısal Olasılıklı Modeller\n",
    "Verilerin nasıl yapısal olarak modellenebileceği ve karmaşık ilişkilerin nasıl temsil edilebileceği hakkında bilgi verir. Bu bölümde, özellikle gizli değişken modelleri ve graf yapıları üzerinden olasılık hesaplamaları anlatılır.\n",
    "\n",
    "Genel Yorum:\n",
    "Bu bölüm, derin öğrenmenin belirsizlikleri nasıl modellediğini anlamak için önemli olasılık teorisi ve bilgi teorisi kavramlarını açıklar. Olasılık teorisi, verilerdeki belirsizlikleri yönetirken; bilgi teorisi, bir modelin ne kadar bilgi taşıdığını ölçmek ve veriyi nasıl en iyi şekilde sıkıştırabileceğimizi anlamak için kullanılır. Derin öğrenme modellerini geliştirirken, bu kavramlar veri ve modeller arasındaki ilişkiyi anlamakta kritik rol oynar.;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c3e83-b4ed-492b-bb19-7616a9eb5cdc",
   "metadata": {},
   "source": [
    "## 4. Practical Methodology (Pratik Yöntemler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f4d38-f958-488d-8dc7-d5d0bd2ccef1",
   "metadata": {},
   "source": [
    "## 5. Applications (Uygulamalar)\n",
    "Büyük Ölçekli Derin Öğrenme: Derin öğrenmenin büyük veri kümeleri üzerindeki performansı ve uygulama alanları (görsel tanıma, konuşma tanıma, doğal dil işleme) anlatılıyor.\n",
    "Bilgisayarla Görme: Görüntü ve video işleme için kullanılan derin öğrenme modelleri.\n",
    "Diğer Uygulamalar: Sağlık, finans, biyoinformatik gibi alanlardaki derin öğrenme uygulamaları detaylandırılıyor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6993a9-a03d-4df8-894d-451197e5c9ba",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Research (Derin Öğrenme Araştırmaları)\n",
    "Doğrusal Faktör Modelleri: Temel faktör analiz yöntemleri, öz bileşen analizi gibi modeller işleniyor.\n",
    "Otoenkodlayıcılar: Özellikle veri boyutlarını küçültmek ve temsil gücünü artırmak için kullanılan otoenkodlayıcı mimariler anlatılıyor.\n",
    "Temsili Öğrenme: Derin öğrenme modellerinin etkili bir şekilde temsil öğrenme kapasiteleri üzerinde duruluyor.\n",
    "Monte Carlo Yöntemleri: Örnekleme teknikleri ve Monte Carlo yöntemlerinin nasıl kullanıldığına dair detaylar.\n",
    "Bu kitabı okurken, özellikle dikkat etmen gereken noktalar:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7f359-f37c-4079-9741-c53a094c54c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfec36e9-b928-47c0-82b1-cde3c0b76575",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b5f5878-c88f-4f41-8100-9f6ae8f251f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6991e52a-d6ad-468a-88a1-b399773e3284",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f087a8-1eca-46de-9a46-2cc064871cbb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11b0428d-0746-4f59-bd2e-e5b73bb8bc65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aaa4582-9b9b-407c-9a0b-d21f2c044645",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949562b8-c89f-4e11-aac4-31727aa46a79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37a4993e-8581-4f33-b3c6-38d03bcc35b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946b72d1-977d-4d4b-b5be-1f4d6b3a86d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7194d5f-48dc-4a08-9d96-30c8d9c55bd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9edb41d-a62d-4d3a-8975-768e6db158cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d775d52b-0251-40f1-adf1-793d9083af7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b502c728-b6e0-4649-ad0b-d3c32be0fc96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e57191-cd0f-43a9-bfaa-b6b0f5a6c6b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd2f4fa-c650-464b-9cc6-2adfb5aa92d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b7500cf-94ca-4ffe-9132-7ee6314ee71e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd7b9db0-1950-4b73-ab7a-6ed65f33f931",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e642fc7a-ea11-4050-a8ff-bf2c61821d88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98448397-5d5e-41d8-a531-07fb6c896a09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e093ed-1739-4091-be31-20e5f2843598",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b05ba9c7-642e-4613-a65f-638b0ee18b92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aced18e-f988-4581-949c-2c7e3f17f52d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ffca59-d409-49ad-bb77-500b56dbf699",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8f10ae0-a443-4a69-bc88-081c2ea731dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "254cb38e-cfce-4f89-9f46-8638c6f5ffea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0924f5e-b3cd-4aac-bae6-be7e97f230b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ec2caf-71c9-4287-83bf-105a73d45847",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9df391-4968-478c-90d4-0c7f03dd36a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6defcce-8d3f-4584-9a3f-8b9a3ec73c4c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
